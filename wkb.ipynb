{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear models\n",
    "1. a linear model is a model where each row is calculated as \n",
    "2. sum(row * weights)\n",
    "3. where weights is learned from the data\n",
    "4. and is the same for every row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = random((30,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = np.dot(x, [2, 3]) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is _not_ our model - this is our \"data\" - a set of relationships between the input x and output y. \n",
    "\n",
    "We're going to pretend that we don't know the transformation function (y = x [2,3] + 1) and try to get the same y outputs from x using the linear model we're about to build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lm = Sequential([Dense(1, input_dim=2)])\n",
    "\n",
    "# lm is the linear model\n",
    "\n",
    "# lm = Sequential()\n",
    "# lm.add(Dense(x, y))\n",
    "# is the same as \n",
    "# lm = Sequential([Dense(x, input_shape=(y,)])\n",
    "# is the same as \n",
    "# lm = Sequential([Dense(x, input_dim=y)])\n",
    "# ... I think\n",
    "# 'dim' stands for 'dimensions' btw because it took me a while\n",
    "\n",
    "# the docs describe Dense() as 'just your fully connected NN layer'\n",
    "\n",
    "# the model will take as input arrays of shape (*, y)\n",
    "# and output arrays of shape(*, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. lm is actually our linear model\n",
    "2. it takes as input arrays of shape (\\*, 2)\n",
    "3. it outputs arrays of shape (\\*, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 2)\n",
      "(30,)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Oh look - what a coincidence!\n",
    "\n",
    "Okay so we have a linear model that takes the right sized inputs and produces the right sized outputs. What exactly is it doing? Just multiplying our inputs by random matrices? **Check on this.**\n",
    "\n",
    "Sometimes there'a an activation step here, but not this time.\n",
    "\n",
    "```\n",
    "lm.add(Dense(?, activation='softmax')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lm.compile(optimizer=SGD(lr=0.1), loss=\"mse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we're going to tell our model how we want it to improve its guesses. And we're saying, use stochastic gradient descent with a learning rate of 0.1 (we'll cover this later). \n",
    "\n",
    "And we need some way to keep score of whether our model is getting better or worse, and we're saying, use mean squared error (we'll cover this later too).\n",
    "\n",
    "**Ok model, give us an error score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29.163684844970703"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.evaluate(x, y, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this good or bad? The Wikipedia page for mean squared error says \"values closer to zero are better\" so I'm going to take that as \"could be improved\".\n",
    "\n",
    "This shouldn't be surprising though - if you've been paying attention you'll realize *the model hasn't actually been trained*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "30/30 [==============================] - 0s - loss: 2.2037     \n",
      "Epoch 2/5\n",
      "30/30 [==============================] - 0s - loss: 0.2884     \n",
      "Epoch 3/5\n",
      "30/30 [==============================] - 0s - loss: 0.1455     \n",
      "Epoch 4/5\n",
      "30/30 [==============================] - 0s - loss: 0.0845     \n",
      "Epoch 5/5\n",
      "30/30 [==============================] - 0s - loss: 0.0424     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2d3ba795d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.fit(x, y, nb_epoch=5, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We told our model to go over the data 5x (**what does that mean exactly?**) - notice the diminishing loss function on the side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.023221131414175034"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.evaluate(x, y, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ok model, do it five more times.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "30/30 [==============================] - 0s - loss: 0.0222     \n",
      "Epoch 2/5\n",
      "30/30 [==============================] - 0s - loss: 0.0118     \n",
      "Epoch 3/5\n",
      "30/30 [==============================] - 0s - loss: 0.0064     \n",
      "Epoch 4/5\n",
      "30/30 [==============================] - 0s - loss: 0.0036     \n",
      "Epoch 5/5\n",
      "30/30 [==============================] - 0s - loss: 0.0020     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2d3ba763d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.fit(x, y, nb_epoch=5, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0015310090966522694"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.evaluate(x, y, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, that's a lot closer to 0!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ok model, what function did you use to produce this output?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.9352],\n",
       "        [ 2.9023]], dtype=float32), array([ 1.1161], dtype=float32)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember our original y function?\n",
    "```\n",
    "y = np.dot(x, [2, 3]) + 1\n",
    "```\n",
    "The weights used by our model turned out pretty close!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
