{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wk_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = wk_dir + \"/../data/kg/cd-redux/sample/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_dir = data_dir + \"/models/\"\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning from scratch(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg = Vgg16()\n",
    "model = vgg.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 images belonging to 2 classes.\n",
      "Found 50 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(data_dir + \"/train/\", shuffle=False, batch_size=1)\n",
    "val_batches = get_batches(data_dir + \"/valid/\", shuffle=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a batch_size of 1 doesn't mean we're not getting all the images - we are.\n",
    "\n",
    "**What is batch_size?**\n",
    "\n",
    "Number of images to process at once. We try to process as many images at a time as our graphics card allows.\n",
    "\n",
    "Apparently when we use a batch_size of 1 it's because we're doing preprocessing on the CPU (??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bcolz\n",
    "def save_array(fname, arr):\n",
    "    c = bcolz.carray(arr, rootdir=fname, mode=\"w\")\n",
    "    c.flush()\n",
    "def load_array(fname):\n",
    "    return bcolz.open(fname)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "val_data = get_data(data_dir + \"/valid/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "trn_data = get_data(data_dir + \"/train/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def get_data(path, target_size=(224,224)):\n",
    "    batches = get_batches(\n",
    "        path, \n",
    "        shuffle=False, \n",
    "        batch_size=1, \n",
    "        class_mode=None, \n",
    "        target_size=target_size\n",
    "    )\n",
    "    return np.concatenate(\n",
    "        [batches.next() for i in range(batches.nb_sample)]\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 153.,  153.,  154., ...,  157.,  159.,  162.],\n",
       "         [ 153.,  153.,  154., ...,  157.,  159.,  162.],\n",
       "         [ 149.,  149.,  150., ...,  151.,  153.,  155.],\n",
       "         ..., \n",
       "         [ 251.,  251.,  252., ...,  251.,  249.,  248.],\n",
       "         [ 251.,  251.,  252., ...,  251.,  249.,  248.],\n",
       "         [ 251.,  251.,  252., ...,  251.,  249.,  248.]],\n",
       "\n",
       "        [[ 193.,  193.,  194., ...,  198.,  200.,  203.],\n",
       "         [ 193.,  193.,  194., ...,  198.,  200.,  203.],\n",
       "         [ 189.,  189.,  190., ...,  192.,  194.,  196.],\n",
       "         ..., \n",
       "         [ 255.,  255.,  255., ...,  248.,  246.,  245.],\n",
       "         [ 255.,  255.,  255., ...,  248.,  246.,  245.],\n",
       "         [ 255.,  255.,  255., ...,  248.,  246.,  245.]],\n",
       "\n",
       "        [[ 255.,  255.,  255., ...,  255.,  255.,  255.],\n",
       "         [ 255.,  255.,  255., ...,  255.,  255.,  255.],\n",
       "         [ 251.,  251.,  252., ...,  255.,  255.,  255.],\n",
       "         ..., \n",
       "         [ 255.,  255.,  255., ...,  255.,  253.,  252.],\n",
       "         [ 255.,  255.,  255., ...,  255.,  253.,  252.],\n",
       "         [ 255.,  255.,  255., ...,  255.,  253.,  252.]]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 112.,  119.,  128., ...,  128.,  127.,  127.],\n",
       "         [ 119.,  128.,  136., ...,  133.,  127.,  127.],\n",
       "         [ 127.,  136.,  144., ...,  141.,  127.,  127.],\n",
       "         ..., \n",
       "         [ 123.,  128.,  132., ...,  136.,  127.,  127.],\n",
       "         [ 120.,  125.,  132., ...,  135.,  127.,  127.],\n",
       "         [ 127.,  127.,  127., ...,  127.,  127.,  127.]],\n",
       "\n",
       "        [[   3.,    2.,    3., ...,    6.,    0.,    0.],\n",
       "         [   2.,    0.,    1., ...,    2.,    0.,    0.],\n",
       "         [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "         ..., \n",
       "         [   4.,    2.,    0., ...,    0.,    0.,    0.],\n",
       "         [   4.,    2.,    0., ...,    0.,    0.,    0.],\n",
       "         [   0.,    0.,    0., ...,    0.,    0.,    0.]],\n",
       "\n",
       "        [[ 255.,  255.,  245., ...,  241.,  255.,  255.],\n",
       "         [ 255.,  255.,  251., ...,  244.,  255.,  255.],\n",
       "         [ 255.,  255.,  255., ...,  253.,  255.,  255.],\n",
       "         ..., \n",
       "         [ 254.,  252.,  249., ...,  255.,  255.,  255.],\n",
       "         [ 255.,  255.,  252., ...,  236.,  255.,  255.],\n",
       "         [ 255.,  255.,  255., ...,  255.,  255.,  255.]]]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 3, 224, 224)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(model_dir + \"train_data.bc\", trn_data)\n",
    "save_array(model_dir + \"valid_data.bc\", val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this necessary? The notes say \"loading and resizing the images every time we want to use them isn't necessary\" - instead we should save the processed arrays. What processing was done though?\n",
    "\n",
    "Seems like all we did up to this point was get_batches and concatenate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def onehot(x):\n",
    "    return np.array(\n",
    "        OneHotEncoder().fit_transform(\n",
    "            x.reshape(-1,1)\n",
    "        ).todense()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_classes = val_batches.classes\n",
    "trn_classes = batches.classes\n",
    "val_labels = onehot(val_classes)\n",
    "trn_labels = onehot(trn_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're formatting the cats and dogs data into a one hot encoded format that we can use for the linear model (I guess)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_features = model.predict(trn_data, batch_size=100)\n",
    "val_features = model.predict(val_data, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Original classes (from data)\n",
    "```\n",
    "batches = get_batches(data_dir + \"/train/\", shuffle=False, batch_size=1)\n",
    "trn_classes = batches.classes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_classes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### One hot encoded labels for our linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Predictions from our VGG model using ImageNet classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.3189e-08,   3.3747e-06,   2.3458e-06,   1.8934e-06,   2.3422e-07,   8.1195e-06,\n",
       "         9.3792e-08,   3.8643e-07,   1.2771e-07,   6.5615e-08], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_features[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 1000)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that every prediction contains 1,000 probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(model_dir + \"train_lastlayer_features.bc\", trn_features)\n",
    "save_array(model_dir + \"valid_lastlayer_features.bc\", val_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the ImageNet class predictions so we don't have to run it again later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lm = Sequential(\n",
    "    [Dense(2, input_shape=(1000,), activation=\"softmax\")]\n",
    ")\n",
    "lm.compile(\n",
    "    optimizer=RMSprop(lr=0.1), \n",
    "    loss=\"categorical_crossentropy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You actually know this one! A linear model with a single NN layer that takes an input (1,000 ImageNet predictions) and produces an output (1 or 0 for cat or dog)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 50 samples\n",
      "Epoch 1/3\n",
      "200/200 [==============================] - 0s - loss: 0.2756 - val_loss: 0.2695\n",
      "Epoch 2/3\n",
      "200/200 [==============================] - 0s - loss: 0.2057 - val_loss: 0.2317\n",
      "Epoch 3/3\n",
      "200/200 [==============================] - 0s - loss: 0.1647 - val_loss: 0.2080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f747fa35990>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.fit(\n",
    "    trn_features,\n",
    "    trn_labels,\n",
    "    nb_epoch=3,\n",
    "    batch_size=64,\n",
    "    validation_data=(\n",
    "        val_features,\n",
    "        val_labels\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_7 (Dense)                  (None, 2)             2002        dense_input_1[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 2002\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has a single dense layer - not sure what \"params\" means here or \"connected to\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
